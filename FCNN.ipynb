{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력값 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5399, 0.1400, 0.1830,  ..., 0.4633, 0.1359, 0.9352],\n",
      "        [0.0200, 0.8462, 0.6503,  ..., 0.4443, 0.2504, 0.7974],\n",
      "        [0.7638, 0.5563, 0.2580,  ..., 0.9433, 0.8098, 0.6860],\n",
      "        ...,\n",
      "        [0.2557, 0.9092, 0.9402,  ..., 0.4886, 0.4394, 0.2635],\n",
      "        [0.3950, 0.4082, 0.8927,  ..., 0.4459, 0.4952, 0.0993],\n",
      "        [0.8111, 0.5117, 0.1676,  ..., 0.3435, 0.3051, 0.5428]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32     # 배치 크기 설정\n",
    "feature_size = 128  # input feature 크기 설정\n",
    "\n",
    "x = torch.rand(batch_size, feature_size)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected NN Layer\n",
    "\n",
    "- 클래스 이름이 Linear인 것은 FC Layer는 Weight와 bias에 대한 선형결합인 것이다. \n",
    "- $W \\cdot x + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 128\n",
    "out_features = 64\n",
    "layer = torch.nn.Linear(in_features = in_features,\n",
    "                        out_features = out_features,\n",
    "                        bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(layer.weight.shape)\n",
    "print(layer.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = layer(x)\n",
    "output2 = x @ layer.weight.T + layer.bias\n",
    "\n",
    "#print(output1)\n",
    "#print(output2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "act_fn1 = torch.nn.Sigmoid()\n",
    "act_fn2 = torch.nn.ReLU()\n",
    "act_fn3 = torch.nn.Tanh()\n",
    "act_fn4 = torch.nn.Softmax()\n",
    "\n",
    "\n",
    "out1 = act_fn1(x)\n",
    "out2 = act_fn2(x)\n",
    "out3 = act_fn3(x)\n",
    "out4 = act_fn4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Layer\n",
    "in_features = 128\n",
    "out_features = 64\n",
    "layer1 = torch.nn.Linear(in_features = in_features,\n",
    "                         out_features = out_features,\n",
    "                         bias = True)\n",
    "\n",
    "act_fn1 = torch.nn.Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Layer\n",
    "in_features = 64\n",
    "out_features = 16\n",
    "layer2 = torch.nn.Linear(in_features = in_features,\n",
    "                         out_features = out_features,\n",
    "                         bias = True)\n",
    "\n",
    "act_fn2 = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third Layer\n",
    "in_features = 16\n",
    "out_features = 1\n",
    "layer3 = torch.nn.Linear(in_features = in_features,\n",
    "                         out_features = out_features,\n",
    "                         bias = True)\n",
    "\n",
    "act_fn3 = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = act_fn1(layer1(x))\n",
    "out_2 = act_fn2(layer2(out_1))\n",
    "out_3 = act_fn3(layer3(out_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Seqeuntial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential은 여러 개의 Layer를 연속적으로 통과하여 한 번에 forward pass를 할 수 있도록 해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nn.Sequential\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    #First Layer\n",
    "    torch.nn.Linear(in_features = 128,\n",
    "                    out_features = 64,\n",
    "                    bias = True),\n",
    "\n",
    "    torch.nn.Tanh(),\n",
    "    #Second Layer\n",
    "    torch.nn.Linear(in_features = 64,\n",
    "                    out_features = 16,\n",
    "                    bias = True), \n",
    "\n",
    "    torch.nn.ReLU(),\n",
    "    #Third Layer\n",
    "    torch.nn.Linear(in_features = 16,\n",
    "                    out_features = 1,\n",
    "                    bias = True),\n",
    "\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module\n",
    "nn.Module을 통해서 신경망을 만들 수 있다. \n",
    "\n",
    "#### nn.Module의 특징\n",
    " - nn.Module을 상속받은 모든 클래스는 신경망 모델이 된다\n",
    " - 신경망 모델은 GPU로 이동할 수 있고, 내보내기와 불러오기가 가능하며 복합적 구조를 형성할 수 있다\n",
    " - nn.Module은 Parameter를 포함하는데, 이는 학습 중에 최적화되는 가중치와 편향이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. init : 신경망 모듈에서 사용되는 모든 모듈을 정의한다 \n",
    "2. forward : 신경망 모듈에서 사용되는 모든 연산을 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Neural Network를 구성하는 Layer을 Initialize 하는 부분\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features = 128,\n",
    "                            out_features = 64,\n",
    "                            bias = True),\n",
    "\n",
    "            torch.nn.Tanh(),\n",
    "        \n",
    "            torch.nn.Linear(in_features = 64,\n",
    "                            out_features = 16,\n",
    "                            bias = True), \n",
    "\n",
    "            torch.nn.ReLU(),\n",
    "   \n",
    "            torch.nn.Linear(in_features = 16,\n",
    "                            out_features = 1,\n",
    "                            bias = True),\n",
    "\n",
    "            torch.nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 두 개의 클래스를 사용한 이유\n",
    "\n",
    "1. 모듈화 및 재사용성 : NeuralNetwork 클래스는 기본적으로 다양한 신경망을 정의하기 위한 템플릿 역할을 할 수 있다\n",
    "2. 유연성 : NeuralNetwork 클래스를 사용하면 여러 모델을 상속받아 만들 수 있기 때문에, 코드의 유연성과 확장성이 증가\n",
    "3. 유지보수성 : 신경망 구조를 변경하거나 추가할 때, 각각의 모델을 개별적으로 정의하면 코드의 중복이 발생할 수 있는데, NeuralNetwork 클래스가 이러한 중복을 줄여준다.\n",
    "\n",
    "\n",
    "#### Model Class만 사용해도 되나?\n",
    "\n",
    "된다. 여기서는 단일 모델만을 정의하고 있기 때문에, 굳이 Neural Network Class를 함께 구성할 필요 없다. 다만, 복잡한 모델을 정의하는 경우, 모듈화된 설계로 여러 모델을 효율적으로 관리할 수 있고, 코드의 재사용성과 확장성을 높일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]           8,256\n",
      "              Tanh-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 16]           1,040\n",
      "              ReLU-4                   [-1, 16]               0\n",
      "            Linear-5                    [-1, 1]              17\n",
      "           Sigmoid-6                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 9,313\n",
      "Trainable params: 9,313\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (128,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
